{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "500c6acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-31 20:33:11.596764: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-31 20:33:11.596809: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on all addresses.\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      " * Running on http://192.168.29.123:8080/ (Press CTRL+C to quit)\n",
      "192.168.29.123 - - [31/May/2022 20:33:36] \"GET / HTTP/1.1\" 200 -\n",
      "192.168.29.123 - - [31/May/2022 20:33:36] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "192.168.29.123 - - [31/May/2022 20:33:38] \"GET /recommendation HTTP/1.1\" 200 -\n",
      "192.168.29.123 - - [31/May/2022 20:33:40] \"POST /recommend HTTP/1.1\" 200 -\n",
      "192.168.29.123 - - [31/May/2022 20:33:43] \"GET /index HTTP/1.1\" 200 -\n",
      "/home/ruthvik/anaconda3/lib/python3.9/site-packages/sklearn/base.py:566: FutureWarning: Arrays of bytes/strings is being converted to decimal numbers if dtype='numeric'. This behavior is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26). Please convert your data to numeric values explicitly instead.\n",
      "  X = check_array(X, **check_params)\n",
      "/home/ruthvik/anaconda3/lib/python3.9/site-packages/sklearn/base.py:566: FutureWarning: Arrays of bytes/strings is being converted to decimal numbers if dtype='numeric'. This behavior is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26). Please convert your data to numeric values explicitly instead.\n",
      "  X = check_array(X, **check_params)\n",
      "192.168.29.123 - - [31/May/2022 20:34:13] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, jsonify, request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import regex as re\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('vader_lexicon')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import gensim\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer,StandardScaler,MinMaxScaler\n",
    "import joblib\n",
    "from scipy.sparse import hstack\n",
    "import spacy\n",
    "\n",
    "import flask\n",
    "app = Flask(__name__)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer('english')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "stop_words.remove('no')\n",
    "entity_keys = ['TIME','WORK_OF_ART']\n",
    "ner_lst = nlp.pipe_labels['ner']\n",
    "\n",
    "\n",
    "#loading the pretrained models\n",
    "clf2 = joblib.load('Bow_model.pkl')\n",
    "clf3 = joblib.load('tfidf_model.pkl')\n",
    "clf4 = joblib.load('ngram_bow_model.pkl')\n",
    "clf5 = joblib.load('ngram_tfidf_model.pkl')\n",
    "clf6 = joblib.load('W2V Model.pkl')\n",
    "\n",
    "#loading the predefined vectorizers\n",
    "vectorizer_bow_1  = joblib.load('vectorizer_bow.pkl')\n",
    "vectorizer_tfidf_1 = joblib.load('vectorizer_tfidf.pkl')\n",
    "vectorizer_bow_n = joblib.load('ngram_vec_bow.pkl')\n",
    "vectoizer_tfidf_n = joblib.load('ngram_vec_tfidf.pkl')\n",
    "vectorizer_w2v = joblib.load('word2vec.bin')\n",
    "\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "\n",
    "def preprocess_text(text_data):\n",
    "    \n",
    "    text_data = decontracted(text_data)\n",
    "    \n",
    "    text_data = text_data.replace('\\n',' ')\n",
    "    text_data = text_data.replace('\\r',' ')\n",
    "    text_data = text_data.replace('\\t',' ')\n",
    "    text_data = text_data.replace('-',' ')\n",
    "    text_data = text_data.replace(\"/\",' ')\n",
    "    text_data = text_data.replace(\">\",' ')\n",
    "    text_data = text_data.replace('\"',' ')\n",
    "    text_data = text_data.replace('?',' ')\n",
    "    return text_data\n",
    "\n",
    "\n",
    "def nlp_preprocessing(review):\n",
    "    '''This functional block preprocess the text data by removing digits, extra spaces, stop words \n",
    "    and converting words to lower case and stemming words'''\n",
    "    \n",
    "    # loading stop words from nltk library\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = SnowballStemmer('english')\n",
    "\n",
    "    #removing 'no' from the stop words list as there is an importance of 'side effects' and 'no side effects' in review\n",
    "    stop_words.remove('no')\n",
    "    \n",
    "    if type(review) is not int:\n",
    "        string = \"\"\n",
    "        review = preprocess_text(review)\n",
    "        review = re.sub('[^a-zA-Z]', ' ', review)\n",
    "        \n",
    "        review = re.sub('\\s+',' ', review)\n",
    "        \n",
    "        review = review.lower()\n",
    "        \n",
    "        for word in review.split():\n",
    "        \n",
    "            if not word in stop_words:\n",
    "                word = stemmer.stem(word)\n",
    "                string += word + \" \"\n",
    "        \n",
    "        return string \n",
    "\n",
    "\n",
    "def get_sentiment_score(review,cleaned_review):\n",
    "\n",
    "    rev_score = sid.polarity_scores(review)['compound']\n",
    "    clean_rev_score = sid.polarity_scores(review)['compound']\n",
    "    \n",
    "    return rev_score,clean_rev_score\n",
    "\n",
    "\n",
    "def get_extracted_features(review,cleaned_review):\n",
    "    \n",
    "    #reference from quora question pair case study\n",
    "\n",
    "    #Word count in each review\n",
    "    word_count =  len(str(cleaned_review).split())\n",
    "\n",
    "    #Unique word count \n",
    "    unique_word_count = len(set(str(cleaned_review).split()))\n",
    "\n",
    "    #character count\n",
    "    char_length = len(str(cleaned_review))\n",
    "\n",
    "    #punctuation count\n",
    "    count_punctuations = len([c for c in str(review) if c in string.punctuation])\n",
    "\n",
    "\n",
    "    #Number of stopwords\n",
    "    stopword_count = len([w for w in str(review).lower().split() if w in stop_words])\n",
    "\n",
    "    #Average length of the words\n",
    "    mean_word_len = np.mean([len(w) for w in str(cleaned_review).split()])\n",
    "    \n",
    "    return word_count,unique_word_count,char_length,count_punctuations,stopword_count,mean_word_len\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def ner(review):\n",
    "\n",
    "    sent = review\n",
    "    doc=nlp(sent)\n",
    "    dic = {}.fromkeys(ner_lst,0)\n",
    "    for word in doc.ents:\n",
    "        dic[word.label_]+=1\n",
    "        \n",
    "    return dic\n",
    "\n",
    "def get_topic_modelling_features(review):\n",
    "    lst =[]\n",
    "    lst.append(review.split())\n",
    "    id2word = gensim.corpora.Dictionary(lst) ## map words to an id\n",
    "    dic_corpus = [id2word.doc2bow(word) for word in lst] ## create dictionary word:freq\n",
    "    ## train LDA\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=dic_corpus, id2word=id2word, num_topics=20, chunksize=100, passes=10, alpha='auto', per_word_topics=True)\n",
    "    \n",
    "    top_topics = (lda_model.get_document_topics(dic_corpus[0], minimum_probability=0.0))\n",
    "    topic_vec = [top_topics[i][1] for i in range(20)]\n",
    "\n",
    "    \n",
    "    return topic_vec\n",
    "\n",
    "\n",
    "\n",
    "def normalize_num_features(features):\n",
    "    normalizer = Normalizer()\n",
    "    num_feat = normalizer.fit_transform(features)\n",
    "    \n",
    "    return num_feat\n",
    "\n",
    "def create_w2v(review,model):\n",
    "    '''This function creates the w2v embeddings for the cleaned reviews passed'''\n",
    "    w2v_vector =[]\n",
    "    vector = np.zeros(300)\n",
    "    for word in review.split():\n",
    "        if word in model.wv.key_to_index:\n",
    "            vector += model.wv[word]\n",
    "    \n",
    "    w2v_vector.append(vector)\n",
    "    w2v_vector = np.array(w2v_vector)\n",
    "    return w2v_vector\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return flask.render_template('home.html')\n",
    "\n",
    "\n",
    "@app.route('/index')\n",
    "def index():\n",
    "    return flask.render_template('index.html')\n",
    "\n",
    "@app.route('/recommendation')\n",
    "def recommendation():\n",
    "    return flask.render_template('recommendation.html')\n",
    "\n",
    "\n",
    "@app.route('/recommend', methods=['POST'])\n",
    "   \n",
    "    \n",
    "def recommend():\n",
    "    '''this function takes the given condition as input and returns the top drugs based on highest rec scores'''\n",
    "    to_predict_list = request.form.to_dict()\n",
    "    condition = to_predict_list['condition']\n",
    "    data = pd.read_csv('validation_data.csv')\n",
    "    select = data[data['condition']==condition].sort_values(by=['rec_score'],ascending=False)\n",
    "    group_drug = select.groupby(['drugName']).agg({'rec_score':['sum']})\n",
    "    group_drug = group_drug[('rec_score', 'sum')].sort_values(ascending=False)\n",
    "    drug_score = dict(group_drug)\n",
    "    drugs =[]\n",
    "    if len(drug_score)>5:\n",
    "        for i in list(drug_score.keys())[0:5]:\n",
    "            drugs.append(i)\n",
    "        return flask.render_template('drugs.html',table=drugs) \n",
    "            \n",
    "    else:\n",
    "        for i in drug_score.keys():\n",
    "            drugs.append(i)\n",
    "            \n",
    "        return flask.render_template('drugs.html',table=drugs)\n",
    "    \n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    # reading the input data\n",
    "    to_predict_list = request.form.to_dict()\n",
    "    review = to_predict_list['review_text']\n",
    "    condition = to_predict_list['condition']\n",
    "    year = int(to_predict_list['year'])\n",
    "    usefulcount = np.array(to_predict_list['usefulcount']).reshape(1,-1)\n",
    "    \n",
    "    #preprocessing the review_text\n",
    "    cleaned_review = nlp_preprocessing(review)\n",
    "    scores = np.array(get_sentiment_score(review,cleaned_review)).reshape(1,-1)\n",
    "    extracted_features  = np.array(get_extracted_features(review,cleaned_review)).reshape(1,-1)\n",
    "    entities = np.array([ner(cleaned_review).get(key) for key in entity_keys]).reshape(1,-1)\n",
    "    topics = get_topic_modelling_features(cleaned_review)\n",
    "    del(topics[9])\n",
    "    del(topics[14])\n",
    "    topics = np.array(topics).reshape(1,-1)\n",
    "    \n",
    "    #normalizing and concatenating numerical features \n",
    "    num_features = np.concatenate((usefulcount,extracted_features,entities,topics),axis=1)\n",
    "    norm_features = normalize_num_features(num_features)\n",
    "    num_features = np.concatenate((norm_features,scores),axis=1)\n",
    "    \n",
    "    #encoding categorical features \n",
    "    label_con = joblib.load('condition_encoder.pkl')\n",
    "    condition = np.array(label_con.transform([condition])).reshape(1,-1)\n",
    "    label_year = joblib.load('year_encoder.pkl')\n",
    "    year = np.array(label_year.transform([year])).reshape(1,-1)\n",
    "    \n",
    "    #loading the predefined vectorizers\n",
    "#     vectorizer_bow_1  = joblib.load('vectorizer_bow.pkl')\n",
    "#     vectorizer_tfidf_1 = joblib.load('vectorizer_tfidf.pkl')\n",
    "#     vectorizer_bow_n = joblib.load('ngram_vec_bow.pkl')\n",
    "#     vectoizer_tfidf_n = joblib.load('ngram_vec_tfidf.pkl')\n",
    "#     vectorizer_w2v = joblib.load('word2vec.bin')\n",
    "    \n",
    "    #transforming the cleaned review\n",
    "    vec_bow_1 = vectorizer_bow_1.transform([cleaned_review])\n",
    "    vec_tfidf_1 = vectorizer_tfidf_1.transform([cleaned_review])\n",
    "    vec_bow_n = vectorizer_bow_n.transform([cleaned_review])\n",
    "    vec_tfidf_n = vectoizer_tfidf_n.transform([cleaned_review])\n",
    "    vec_w2v = create_w2v(cleaned_review, vectorizer_w2v)\n",
    "    \n",
    "    # concatenating all the features     \n",
    "    vector2 = hstack((num_features,condition,year,vec_bow_1)).tocsr()\n",
    "    vector3 = hstack((num_features,condition,year,vec_tfidf_1)).tocsr()\n",
    "    vector4 = hstack((num_features,condition,year,vec_bow_n)).tocsr()\n",
    "    vector5 = hstack((num_features,condition,year,vec_tfidf_n)).tocsr()\n",
    "    vector6 = np.concatenate((num_features,condition,year,vec_w2v),axis=1)\n",
    "    \n",
    "    #loading the pretrained models\n",
    "#     clf2 = joblib.load('Bow_model.pkl')\n",
    "#     clf3 = joblib.load('tfidf_model.pkl')\n",
    "#     clf4 = joblib.load('ngram_bow_model.pkl')\n",
    "#     clf5 = joblib.load('ngram_tfidf_model.pkl')\n",
    "#     clf6 = joblib.load('W2V Model.pkl')\n",
    "    \n",
    " \n",
    "    #predicting the output for given query point\n",
    "    pred =[]\n",
    "    pred.append(clf2.predict(vector2)[0])\n",
    "    pred.append(clf3.predict(vector3)[0])\n",
    "    pred.append(clf4.predict(vector4)[0])\n",
    "    pred.append(clf5.predict(vector5)[0])\n",
    "    pred.append(clf6.predict(vector6)[0])\n",
    "    \n",
    "    \n",
    "    if sum(pred)>=3:\n",
    "        prediction = \"Positive\"\n",
    "    else:\n",
    "        prediction = \"Negative\"\n",
    "\n",
    "    return jsonify({'prediction': prediction})\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=8080)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f165f653",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6b9702",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
